{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456c29cc",
   "metadata": {},
   "source": [
    "## Velocity-Oriented Analytics (Deviation from True Streaming)\n",
    "\n",
    "This notebook demonstrates velocity-oriented data processing through a controlled\n",
    "micro-batch simulation rather than true real-time streaming. While Apache Spark\n",
    "Structured Streaming was initially considered, platform-specific limitations led to\n",
    "the adoption of a bounded micro-batch approach.\n",
    "\n",
    "By processing the dataset incrementally in fixed-size batches and recomputing analytics\n",
    "over time, this approach captures the core principles of data velocity, including\n",
    "incremental computation, temporal ordering, and evolving insights, while ensuring\n",
    "reproducibility and stable execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"ProstateCancerVelocitySimulation\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4524d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(\"..\", \"data\", \"prostate_cancer_prediction.csv\")\n",
    "\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a3fcb",
   "metadata": {},
   "source": [
    "#### Adding synthetic time dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4709d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_time = df.withColumn(\n",
    "    \"arrival_id\",\n",
    "    monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "df_with_time.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e2cf3c",
   "metadata": {},
   "source": [
    "#### Define Micro-Batch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675de10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "MAX_BATCHES = 5   # limits runtime intentionally\n",
    "\n",
    "TOTAL_ROWS = df_with_time.count()\n",
    "\n",
    "print(f\"Total records: {TOTAL_ROWS}\")\n",
    "print(f\"Micro-batch size: {BATCH_SIZE}\")\n",
    "print(f\"Maximum batches to process: {MAX_BATCHES}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f70105",
   "metadata": {},
   "source": [
    "#### Micro-Batch Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b1de6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "import time\n",
    "\n",
    "current_offset = 0\n",
    "batch_number = 1\n",
    "\n",
    "while current_offset < TOTAL_ROWS and batch_number <= MAX_BATCHES:\n",
    "    print(f\"\\n=== Processing Micro-Batch {batch_number} ===\")\n",
    "    print(f\"Time: {time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    micro_batch = (\n",
    "        df_with_time\n",
    "        .filter(\n",
    "            (df_with_time.arrival_id >= current_offset) &\n",
    "            (df_with_time.arrival_id < current_offset + BATCH_SIZE)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    result = micro_batch.groupBy(\"Cancer_Stage\").agg(\n",
    "        avg(\"PSA_Level\").alias(\"avg_psa\")\n",
    "    )\n",
    "    \n",
    "    result.show()\n",
    "    \n",
    "    current_offset += BATCH_SIZE\n",
    "    batch_number += 1\n",
    "    \n",
    "    # Simulate delay between arrivals\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"\\nVelocity simulation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43f1cb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The velocity simulation demonstrates how analytical results can be updated as new data\n",
    "arrives incrementally. By recomputing metrics over successive micro-batches, this\n",
    "notebook illustrates the impact of data velocity on analytics without requiring\n",
    "continuous streaming infrastructure.\n",
    "\n",
    "This approach complements the batch analytics by showing how population-level insights\n",
    "can be observed dynamically over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897249c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
